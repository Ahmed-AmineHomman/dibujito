from __future__ import annotations

import logging
from dataclasses import dataclass
from pathlib import Path
from queue import Queue
from threading import Thread
from typing import Iterator, List, Optional

import gradio as gr
from PIL import Image

from api import Diffuser, LLM, PromptingRules

logger = logging.getLogger(__name__)


@dataclass
class ModelRegistry:
    """Keeps track of the currently loaded backend models"""

    writer: Optional[LLM] = None
    artist: Optional[Diffuser] = None

    def register(self, llm: Optional[LLM] = None, diffuser: Optional[Diffuser] = None) -> None:
        if llm is not None:
            self.writer = llm
        if diffuser is not None:
            self.artist = diffuser

    def require_writer(self) -> LLM:
        if self.writer is None:
            message = "No LLM registered. Call `load_model` during startup to provide one."
            logger.error(message)
            raise RuntimeError(message)
        return self.writer

    def require_artist(self) -> Diffuser:
        if self.artist is None:
            message = "No diffuser registered. Call `load_model` during startup to provide one."
            logger.error(message)
            raise RuntimeError(message)
        return self.artist


MODELS = ModelRegistry()


def load_model(llm: Optional[LLM] = None, diffuser: Optional[Diffuser] = None) -> None:
    """Registers the provided models so downstream helpers can access them at runtime."""
    MODELS.register(llm=llm, diffuser=diffuser)


def get_model_list(directory: str, model_type: str) -> List[str]:
    """Returns the list of supported model files contained in ``directory``."""
    extension_mapper = {
        "llm": ".gguf",
        "diffuser": ".safetensors",
        "optimizer": ".toml",
    }

    if model_type not in extension_mapper:
        message = f"Unsupported model type '{model_type}'."
        logger.error(message)
        raise ValueError(message)

    path = Path(directory)
    if not path.exists():
        message = f"Directory '{directory}' does not exist."
        logger.error(message)
        raise FileNotFoundError(message)

    suffix = extension_mapper[model_type]
    names = sorted(entry.name for entry in path.iterdir() if entry.is_file() and entry.suffix == suffix)
    if not names:
        logger.warning("No '%s' files found in '%s'.", suffix, directory)
    return names


def log(  # noqa: D401 - keep docstring short and focused
    message: str,
    message_type: str = "info",
    progress: Optional[float] = 0.0,
    progressbar: Optional[gr.Progress] = None,
    show_in_ui: bool = False,
) -> None:
    """Log a message and optionally surface it in the Gradio UI."""
    level_handlers = {
        "info": logger.info,
        "warning": logger.warning,
        "error": logger.error,
    }

    if message_type not in level_handlers:
        error_message = f"unknown message type '{message_type}' (supported are 'info', 'warning' and 'error')"
        logger.error(error_message)
        raise gr.Error(error_message)

    level_handlers[message_type](message)
    _notify_ui(message=message, severity=message_type, force=show_in_ui)

    if progressbar is not None:
        progressbar(progress=progress, desc=message)

    if message_type == "error":
        raise gr.Error(message)


def _notify_ui(message: str, severity: str, force: bool) -> None:
    """Render a Gradio notification if needed."""
    display_in_ui = force or severity in {"warning", "error"}
    if not display_in_ui:
        return

    component_name = {
        "info": "Info",
        "warning": "Warning",
    }.get(severity, "Info")

    if severity == "error":
        return

    component = getattr(gr, component_name, None)
    if component is not None:
        component(message)  # type: ignore[callable-async]


def optimize_prompt(
    description: str,
    llm: str,
    llm_dir: str,
    rules: str,
    rules_dir: str,
    *,
    goal: Optional[str] = None,
    temperature: float = 0.5,
    seed: Optional[int] = None,
    creative_mode: bool = True,
) -> Iterator[str]:
    """Stream optimized prompt chunks generated by the configured LLM for the assistant."""
    writer = MODELS.require_writer()
    _load_writer_model(writer=writer, model_dir=llm_dir, model_name=llm)
    rule_set = _load_prompting_rules(directory=rules_dir, rule_name=rules)

    seed_value = None
    if seed is not None:
        try:
            seed_candidate = int(seed)
        except (TypeError, ValueError):
            seed_candidate = -1
        seed_value = seed_candidate if seed_candidate >= 0 else None

    response = writer.optimize_prompt(
        prompt=description,
        goal=goal,
        creative_mode=creative_mode,
        rules=rule_set,
        temperature=temperature,
        seed=seed_value,
    )

    yield from response


def _load_writer_model(writer: LLM, model_dir: str, model_name: str) -> None:
    model_path = Path(model_dir) / model_name
    try:
        writer.load_model(filepath=str(model_path))
    except Exception as error:  # pragma: no cover - surfaced to the UI
        log(message=f"error (llm loading): {error}", message_type="error")


def _load_prompting_rules(directory: str, rule_name: str) -> PromptingRules:
    filepath = Path(directory) / rule_name
    try:
        return PromptingRules.from_toml(filepath=str(filepath))
    except Exception as error:  # pragma: no cover - surfaced to the UI
        log(message=f"error (rules loading): {error}", message_type="error")


def generate_image(
    prompt: str,
    diffuser: str,
    diffuser_dir: str,
    negative_prompt: Optional[str] = None,
    steps: int = 25,
    guidance: float = 7.0,
    preview_frequency: int = 0,
    preview_method: str = "fast",
    aspect: str = "square",
    seed: Optional[int] = -1,
    progressbar: Optional[gr.Progress] = None,
) -> Iterator[Image.Image]:
    """Generate an image (and optional previews) using the configured diffusion pipeline."""
    artist = MODELS.require_artist()

    preview_frequency = _normalise_preview_frequency(preview_frequency)
    preview_method = _normalise_preview_method(preview_method)
    seed_value = seed if seed is not None and seed >= 0 else None

    log(message="loading diffuser", progress=0.25, progressbar=progressbar)
    _load_diffuser_model(artist=artist, model_dir=diffuser_dir, model_name=diffuser)
    log(message="generating image", progress=0.75, progressbar=progressbar)

    try:
        if preview_frequency > 0:
            yield from _generate_image_with_previews(
                artist=artist,
                prompt=prompt,
                negative_prompt=negative_prompt,
                steps=steps,
                guidance=guidance,
                aspect=aspect,
                seed=seed_value,
                preview_frequency=preview_frequency,
                preview_method=preview_method,
                progressbar=progressbar,
            )
        else:
            final_image = artist.imagine(
                prompt=prompt,
                negative_prompt=negative_prompt,
                steps=steps,
                guidance=guidance,
                aspect=aspect,
                seed=seed_value,
                preview_method=preview_method,
            )
            log(message="done", progress=1.0, progressbar=progressbar)
            yield final_image
    except Exception as error:  # pragma: no cover - surfaced to the UI
        log(
            message=f"error (image generation): {error}",
            message_type="error",
            progress=1.0,
            progressbar=progressbar,
        )


def _load_diffuser_model(artist: Diffuser, model_dir: str, model_name: str) -> None:
    model_path = Path(model_dir) / model_name
    try:
        artist.load_model(filepath=str(model_path))
    except Exception as error:  # pragma: no cover - surfaced to the UI
        log(message=f"error (diffuser loading): {error}", message_type="error")


def _generate_image_with_previews(
    artist: Diffuser,
    prompt: str,
    negative_prompt: Optional[str],
    steps: int,
    guidance: float,
    aspect: str,
    seed: Optional[int],
    preview_frequency: int,
    preview_method: str,
    progressbar: Optional[gr.Progress],
) -> Iterator[Image.Image]:
    """Stream intermediate previews while the diffusion pipeline runs."""
    queue: Queue = Queue()
    worker_exception: Optional[Exception] = None

    def handle_preview(image: Image.Image, step: int) -> None:
        queue.put(("preview", image, step))

    def run_pipeline() -> None:
        nonlocal worker_exception
        try:
            final_image = artist.imagine(
                prompt=prompt,
                negative_prompt=negative_prompt,
                steps=steps,
                guidance=guidance,
                aspect=aspect,
                seed=seed,
                preview_frequency=preview_frequency,
                preview_method=preview_method,
                preview_callback=handle_preview,
            )
            queue.put(("final", final_image, steps))
        except Exception as error:  # pragma: no cover - surfaced to the UI
            worker_exception = error
            queue.put(("error", error, -1))
        finally:
            queue.put(("done", None, -1))

    Thread(target=run_pipeline, daemon=True).start()

    while True:
        kind, payload, _step = queue.get()
        if kind == "preview":
            yield payload
        elif kind == "final":
            yield payload
        elif kind == "error":
            if worker_exception is not None:
                log(
                    message=f"error (image generation): {worker_exception}",
                    message_type="error",
                    progress=1.0,
                    progressbar=progressbar,
                )
        elif kind == "done":
            break

    log(message="done", progress=1.0, progressbar=progressbar)


def _normalise_preview_frequency(value: int) -> int:
    try:
        frequency = int(value)
    except (TypeError, ValueError):
        return 0
    return max(0, frequency)


def _normalise_preview_method(method: Optional[str]) -> str:
    if not method:
        return "fast"
    method = method.lower()
    if method not in {"fast", "medium", "full"}:
        log(message=f"unsupported preview method '{method}', defaulting to 'fast'", message_type="warning")
        return "fast"
    return method
