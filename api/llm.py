from __future__ import annotations

import logging
from pathlib import Path
from typing import Dict, Iterator, List, Optional

from llama_cpp import Llama

from .prompting_rules import PromptingRules

logger = logging.getLogger(__name__)

PROMPT_TEMPLATE = """You are a prompt engineer.
You will be provided with an image description (prompt) and a goal from a user of a text-to-image diffusion model.
Craft an optimized prompt that helps the diffusion model generate the most aesthetically pleasing image aligned with the goal.
Return the optimized prompt as plain text only, with no additional commentary.

Here are the prompting rules the diffusion model expects:
{rules}

Follow these additional instructions:
{instructions}

Examples of a good optimized prompt:
{examples}
"""


class LLM:
    """Convenience wrapper around llama.cpp to optimise prompts based on prompting rules."""

    ready: bool
    model_path: str
    llm: Llama

    def __init__(self, filepath: Optional[str] = None) -> None:
        self.ready = False
        self.model_path = ""
        if filepath:
            self.load_model(filepath=filepath)

    def load_model(self, filepath: str) -> None:
        """Load a llama.cpp-compatible model from ``filepath``."""
        if self.ready and filepath == self.model_path:
            logger.info("Skipping LLM load; '%s' already active.", filepath)
            return

        path = self._validate_model_path(filepath)
        self.llm = Llama(model_path=str(path), n_ctx=0)
        self.ready = True
        self.model_path = str(path)

    def optimize_prompt(
        self,
        prompt: str,
        rules: PromptingRules,
        goal: Optional[str] = "Create beautiful and aesthetically pleasing images",
        creative_mode: bool = False,
        **kwargs,
    ) -> Iterator[str]:
        """Stream the optimized prompt generated by the underlying model."""
        instructions = self._build_instruction_block(creative_mode=creative_mode)
        user_prompt = self._build_user_prompt(prompt=prompt, goal=goal if creative_mode else None)
        system_prompt = self._build_system_prompt(rules=rules, instructions=instructions)

        if rules.prefix:
            yield rules.prefix

        for chunk in self._respond(prompt=user_prompt, system_prompt=system_prompt, **kwargs):
            yield chunk

        if rules.suffix:
            yield rules.suffix

    def _respond(self, prompt: str, system_prompt: Optional[str] = None, **kwargs) -> Iterator[str]:
        if not self.ready:
            message = "No model loaded. Please call `load_model` to load a model first."
            logger.error(message)
            raise RuntimeError(message)

        messages: List[Dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.llm.create_chat_completion(messages=messages, stream=True, **kwargs)
        for chunk in response:
            data = chunk["choices"][0]["delta"]
            content = data.get("content")
            if content:
                yield content

    def _build_instruction_block(self, creative_mode: bool) -> str:
        if not creative_mode:
            return (
                "Ensure that your prompt reflects all information contained in the user's description.\n"
                "Do not invent new elements; only rely on the provided description. "
                "If a section cannot be filled, omit it gracefully."
            )
        return (
            "Ensure that your prompt covers every part of the expected structure.\n"
            "When the user description lacks details, invent coherent additions that align with the stated goal."
        )

    def _build_user_prompt(self, prompt: str, goal: Optional[str]) -> str:
        message = f"Description:\n{prompt}"
        if goal:
            message += f"\nGoal:\n{goal}"
        return message

    def _build_system_prompt(self, rules: PromptingRules, instructions: str) -> str:
        examples = "\n".join(rules.examples) if rules.examples else "No examples provided."
        return PROMPT_TEMPLATE.format(
            rules=rules.rules.strip(),
            instructions=instructions.strip(),
            examples=examples.strip(),
        )

    def _validate_model_path(self, filepath: str) -> Path:
        path = Path(filepath)
        if not path.exists():
            message = f"Provided filepath '{filepath}' does not exist."
            logger.error(message)
            raise FileNotFoundError(message)
        if not path.is_file():
            message = f"Provided filepath '{filepath}' is not a file."
            logger.error(message)
            raise IsADirectoryError(message)
        if path.suffix != ".gguf":
            message = "Only GGUF files are supported."
            logger.error(message)
            raise ValueError(message)
        return path

    @staticmethod
    def get_supported_rules() -> list[str]:
        """Expose the available prompting rule presets shipped with the app."""
        rules_dir = Path(__file__).resolve().parent.parent / "data" / "prompting_rules"
        if not rules_dir.exists():
            return []
        return sorted(entry.name for entry in rules_dir.glob("*.toml") if entry.is_file())
